"""
Dev.to API Scraper
ë¯¼ì¤€ í˜ë¥´ì†Œë‚˜: Python/í”„ë¡œê·¸ë˜ë° ì´ˆë³´ì ë¸”ë¡œê·¸ ê¸€ ìˆ˜ì§‘
"""

import requests
import pandas as pd
from datetime import datetime
import time
from tqdm import tqdm
import os

class DevToScraper:
    def __init__(self):
        self.base_url = "https://dev.to/api"
        self.articles = []
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'MinjunResearch/1.0'
        })

    def search_articles(self, tag, per_page=30, num_pages=5):
        """
        íƒœê·¸ë³„ ê¸€ ê²€ìƒ‰

        Args:
            tag: ê²€ìƒ‰ íƒœê·¸ (ì˜ˆ: 'python', 'beginners')
            per_page: í˜ì´ì§€ë‹¹ ê¸€ ìˆ˜
            num_pages: ê²€ìƒ‰ í˜ì´ì§€ ìˆ˜
        """
        print(f"\nğŸ·ï¸  íƒœê·¸ ê²€ìƒ‰: '{tag}'")

        for page in range(1, num_pages + 1):
            params = {
                'tag': tag,
                'per_page': per_page,
                'page': page
            }

            try:
                response = self.session.get(
                    f"{self.base_url}/articles",
                    params=params,
                    timeout=10
                )
                response.raise_for_status()
                articles = response.json()

                for article in articles:
                    article_data = {
                        'search_tag': tag,
                        'title': article.get('title', ''),
                        'description': article.get('description', ''),
                        'url': article.get('url', ''),
                        'published_at': article.get('published_at', ''),
                        'tags': ', '.join(article.get('tag_list', [])),
                        'reactions': article.get('public_reactions_count', 0),
                        'comments': article.get('comments_count', 0),
                        'reading_time': article.get('reading_time_minutes', 0),
                        'user': article.get('user', {}).get('username', ''),
                        'article_id': article.get('id', ''),
                        'collected_at': datetime.now()
                    }
                    self.articles.append(article_data)

                print(f"  í˜ì´ì§€ {page}/{num_pages}: {len(articles)}ê°œ ê¸€")
                time.sleep(1)  # Rate limiting

            except requests.exceptions.RequestException as e:
                print(f"  âŒ ì˜¤ë¥˜ (í˜ì´ì§€ {page}): {str(e)}")
                continue

        print(f"  âœ“ ì´ {len([a for a in self.articles if a['search_tag'] == tag])}ê°œ ìˆ˜ì§‘")

    def search_multiple_tags(self, tags, per_page=30, num_pages=5):
        """ì—¬ëŸ¬ íƒœê·¸ë¡œ ë°ì´í„° ìˆ˜ì§‘"""
        print(f"\n{'='*60}")
        print(f"ğŸ“ Dev.to ê¸€ ìˆ˜ì§‘ ì‹œì‘")
        print(f"{'='*60}")
        print(f"ê²€ìƒ‰ íƒœê·¸: {len(tags)}ê°œ")
        print(f"í˜ì´ì§€ë‹¹: {num_pages}í˜ì´ì§€\n")

        for idx, tag in enumerate(tags, 1):
            print(f"\n[{idx}/{len(tags)}]", end=" ")
            self.search_articles(tag, per_page=per_page, num_pages=num_pages)

        print(f"\n{'='*60}")
        print(f"âœ… ìˆ˜ì§‘ ì™„ë£Œ: ì´ {len(self.articles)}ê°œ ê¸€")
        print(f"{'='*60}\n")

    def to_dataframe(self):
        """DataFrameìœ¼ë¡œ ë³€í™˜"""
        if not self.articles:
            return pd.DataFrame()

        df = pd.DataFrame(self.articles)

        # ì¤‘ë³µ ì œê±°
        original_count = len(df)
        df = df.drop_duplicates(subset=['article_id'])
        removed = original_count - len(df)

        print(f"ğŸ“Š ë°ì´í„° ì •ë¦¬ ì™„ë£Œ: {len(df)}ê°œ ê³ ìœ  ê¸€ (ì¤‘ë³µ {removed}ê°œ ì œê±°)")
        return df

    def save_data(self, filename_prefix='devto_articles'):
        """ë°ì´í„° ì €ì¥"""
        df = self.to_dataframe()

        if df.empty:
            print("âš  ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None, None, None

        os.makedirs('output', exist_ok=True)

        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')

        # CSV ì €ì¥
        csv_path = f"output/{filename_prefix}_{timestamp}.csv"
        df.to_csv(csv_path, index=False, encoding='utf-8-sig')

        # Excel ì €ì¥
        excel_path = f"output/{filename_prefix}_{timestamp}.xlsx"
        df.to_excel(excel_path, index=False, engine='openpyxl')

        print(f"\nğŸ’¾ ë°ì´í„° ì €ì¥ ì™„ë£Œ:")
        print(f"  - CSV: {csv_path}")
        print(f"  - Excel: {excel_path}")

        return csv_path, excel_path, df


def main():
    """ì‹¤í–‰ ì˜ˆì‹œ"""
    # ë¯¼ì¤€ í˜ë¥´ì†Œë‚˜ íƒ€ê²Ÿ íƒœê·¸
    search_tags = [
        'python',
        'beginners',
        'tutorial',
        'learning',
        'programming',
        'datascience',
        'coding',
        'webdev',
        'javascript'  # ë¹„êµë¥¼ ìœ„í•´
    ]

    scraper = DevToScraper()
    scraper.search_multiple_tags(search_tags, per_page=30, num_pages=3)

    # ë°ì´í„° ì €ì¥
    csv_path, excel_path, df = scraper.save_data('minjun_devto')

    if df is not None and not df.empty:
        print(f"\nğŸ“ˆ ìˆ˜ì§‘ ê²°ê³¼ ìš”ì•½:")
        print(f"  - ì´ ê¸€: {len(df)}ê°œ")
        print(f"  - í‰ê·  ë°˜ì‘ ìˆ˜: {df['reactions'].mean():.1f}")
        print(f"  - í‰ê·  ëŒ“ê¸€ ìˆ˜: {df['comments'].mean():.1f}")
        print(f"  - í‰ê·  ì½ê¸° ì‹œê°„: {df['reading_time'].mean():.1f}ë¶„")
        print(f"\nğŸ”¥ ì¸ê¸° ê¸€ (ìƒìœ„ 5ê°œ):")
        top_articles = df.nlargest(5, 'reactions')[['title', 'reactions', 'comments', 'search_tag']]
        for idx, row in top_articles.iterrows():
            print(f"  {row['reactions']:4d}â¤ï¸  | {row['comments']:3d}ğŸ’¬ | [{row['search_tag']}] {row['title'][:50]}")

    return df


if __name__ == "__main__":
    main()
