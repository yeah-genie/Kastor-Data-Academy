"""
Hacker News API Scraper
ë¯¼ì¤€ í˜ë¥´ì†Œë‚˜: Python/í”„ë¡œê·¸ë˜ë° í•™ìŠµ ê´€ë ¨ í† ë¡  ìˆ˜ì§‘
"""

import requests
import pandas as pd
from datetime import datetime
import time
from tqdm import tqdm
import os

class HackerNewsScraper:
    def __init__(self):
        self.base_url = "https://hacker-news.firebaseio.com/v0"
        self.stories = []

    def search_algolia(self, query, tags=None, num_pages=5):
        """
        Algolia HN Search API ì‚¬ìš©

        Args:
            query: ê²€ìƒ‰ í‚¤ì›Œë“œ
            tags: íƒœê·¸ í•„í„° (ì˜ˆ: 'story', 'comment')
            num_pages: ê²€ìƒ‰ í˜ì´ì§€ ìˆ˜
        """
        print(f"\nğŸ” ê²€ìƒ‰ ì¤‘: '{query}'")

        search_url = "http://hn.algolia.com/api/v1/search"

        for page in range(num_pages):
            params = {
                'query': query,
                'tags': tags or 'story',
                'page': page,
                'hitsPerPage': 50
            }

            try:
                response = requests.get(search_url, params=params, timeout=10)
                response.raise_for_status()
                data = response.json()

                hits = data.get('hits', [])

                for hit in hits:
                    story_data = {
                        'search_query': query,
                        'title': hit.get('title', ''),
                        'url': hit.get('url', ''),
                        'author': hit.get('author', ''),
                        'points': hit.get('points', 0),
                        'num_comments': hit.get('num_comments', 0),
                        'created_at': hit.get('created_at', ''),
                        'story_text': hit.get('story_text', ''),
                        'objectID': hit.get('objectID', ''),
                        'hn_url': f"https://news.ycombinator.com/item?id={hit.get('objectID', '')}",
                        'collected_at': datetime.now()
                    }
                    self.stories.append(story_data)

                print(f"  í˜ì´ì§€ {page + 1}/{num_pages}: {len(hits)}ê°œ ìŠ¤í† ë¦¬")
                time.sleep(1)  # Rate limiting

            except requests.exceptions.RequestException as e:
                print(f"  âŒ ì˜¤ë¥˜ (í˜ì´ì§€ {page + 1}): {str(e)}")
                continue

        print(f"  âœ“ ì´ {len([s for s in self.stories if s['search_query'] == query])}ê°œ ìˆ˜ì§‘")

    def search_multiple_queries(self, queries, num_pages=5):
        """ì—¬ëŸ¬ ê²€ìƒ‰ì–´ë¡œ ë°ì´í„° ìˆ˜ì§‘"""
        print(f"\n{'='*60}")
        print(f"ğŸ“° Hacker News ê²€ìƒ‰ ì‹œì‘")
        print(f"{'='*60}")
        print(f"ê²€ìƒ‰ í‚¤ì›Œë“œ: {len(queries)}ê°œ")
        print(f"í˜ì´ì§€ë‹¹: {num_pages}í˜ì´ì§€\n")

        for idx, query in enumerate(queries, 1):
            print(f"\n[{idx}/{len(queries)}]", end=" ")
            self.search_algolia(query, tags='story', num_pages=num_pages)

        print(f"\n{'='*60}")
        print(f"âœ… ê²€ìƒ‰ ì™„ë£Œ: ì´ {len(self.stories)}ê°œ ìŠ¤í† ë¦¬")
        print(f"{'='*60}\n")

    def to_dataframe(self):
        """DataFrameìœ¼ë¡œ ë³€í™˜"""
        if not self.stories:
            return pd.DataFrame()

        df = pd.DataFrame(self.stories)

        # ì¤‘ë³µ ì œê±°
        original_count = len(df)
        df = df.drop_duplicates(subset=['objectID'])
        removed = original_count - len(df)

        print(f"ğŸ“Š ë°ì´í„° ì •ë¦¬ ì™„ë£Œ: {len(df)}ê°œ ê³ ìœ  ìŠ¤í† ë¦¬ (ì¤‘ë³µ {removed}ê°œ ì œê±°)")
        return df

    def save_data(self, filename_prefix='hackernews_stories'):
        """ë°ì´í„° ì €ì¥"""
        df = self.to_dataframe()

        if df.empty:
            print("âš  ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None, None, None

        os.makedirs('output', exist_ok=True)

        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')

        # CSV ì €ì¥
        csv_path = f"output/{filename_prefix}_{timestamp}.csv"
        df.to_csv(csv_path, index=False, encoding='utf-8-sig')

        # Excel ì €ì¥
        excel_path = f"output/{filename_prefix}_{timestamp}.xlsx"
        df.to_excel(excel_path, index=False, engine='openpyxl')

        print(f"\nğŸ’¾ ë°ì´í„° ì €ì¥ ì™„ë£Œ:")
        print(f"  - CSV: {csv_path}")
        print(f"  - Excel: {excel_path}")

        return csv_path, excel_path, df


def main():
    """ì‹¤í–‰ ì˜ˆì‹œ"""
    # ë¯¼ì¤€ í˜ë¥´ì†Œë‚˜ íƒ€ê²Ÿ ê²€ìƒ‰ì–´
    search_queries = [
        'python beginner',
        'learning python',
        'python difficult',
        'learn programming',
        'data science beginner',
        'coding bootcamp',
        'online courses',
        'python tutorial',
        'programming frustration',
        'give up programming'
    ]

    scraper = HackerNewsScraper()
    scraper.search_multiple_queries(search_queries, num_pages=3)

    # ë°ì´í„° ì €ì¥
    csv_path, excel_path, df = scraper.save_data('minjun_hackernews')

    if df is not None and not df.empty:
        print(f"\nğŸ“ˆ ìˆ˜ì§‘ ê²°ê³¼ ìš”ì•½:")
        print(f"  - ì´ ìŠ¤í† ë¦¬: {len(df)}ê°œ")
        print(f"  - í‰ê·  í¬ì¸íŠ¸: {df['points'].mean():.1f}")
        print(f"  - í‰ê·  ëŒ“ê¸€ ìˆ˜: {df['num_comments'].mean():.1f}")
        print(f"\nğŸ”¥ ì¸ê¸° í† í”½ (ìƒìœ„ 5ê°œ):")
        top_stories = df.nlargest(5, 'points')[['title', 'points', 'num_comments']]
        for idx, row in top_stories.iterrows():
            print(f"  {row['points']:4d}ì  | {row['num_comments']:3d}ëŒ“ê¸€ | {row['title'][:60]}")

    return df


if __name__ == "__main__":
    main()
