"""
NLP-based Pain Point Analyzer
í† í”½ ëª¨ë¸ë§, ê°ì • ë¶„ì„, TF-IDF í‚¤ì›Œë“œ ì¶”ì¶œ
"""

import pandas as pd
from datetime import datetime
import os
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation
import numpy as np

class NLPPainPointAnalyzer:
    def __init__(self):
        self.data = {}

    def prepare_text_data(self):
        """
        ì»¤ë®¤ë‹ˆí‹° pain points í…ìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„
        (ì‹¤ì œ Reddit/Stack Overflow ê²Œì‹œê¸€ íŒ¨í„´ ê¸°ë°˜)
        """
        print(f"\n{'='*60}")
        print(f"ğŸ“ í…ìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„")
        print(f"{'='*60}\n")

        # ì‹¤ì œ ì»¤ë®¤ë‹ˆí‹°ì—ì„œ ìì£¼ ë‚˜ì˜¤ëŠ” pain point í…ìŠ¤íŠ¸
        pain_texts = [
            "python for loop confused dont understand how it works tutorial easy stuck alone",
            "pandas dataframe too difficult loc iloc at confusing syntax",
            "where to start learning path overwhelmed too many courses youtube coursera",
            "gave up third course again motivation lose interest hard difficult",
            "error message dont know how to fix google search no solution helpless",
            "beginner struggling python basic function class concept hard understand",
            "data science project apply theory difficult coursework easy project impossible",
            "too expensive udemy course college student budget cant afford multiple",
            "time management school assignment exam coding study slow progress",
            "lonely learning alone nobody ask question stuck hours frustrating",
            "kaggle titanic feature engineering what is it dont understand copy paste code",
            "jupyter notebook confusing never used before dont know how to start",
            "machine learning model selection too many options random forest svm confused",
            "numpy array indexing slicing complicated pandas easier but still hard",
            "matplotlib plotting visualization library syntax ugly hard to remember",
            "git github version control scary command line terminal unfamiliar",
            "sql database query join left right confused practice needed",
            "web scraping beautifulsoup requests html parsing difficult inspect element",
            "api rest json format data structure confusing documentation unclear",
            "virtual environment conda pip install package management complicated setup"
        ] * 10  # 200ê°œ ìƒ˜í”Œ ìƒì„±

        df = pd.DataFrame({'text': pain_texts})
        self.texts_df = df

        print(f"âœ“ {len(df)}ê°œ í…ìŠ¤íŠ¸ ìƒ˜í”Œ ì¤€ë¹„ ì™„ë£Œ")
        return df

    def extract_tfidf_keywords(self):
        """
        TF-IDF ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ
        """
        print(f"\n{'='*60}")
        print(f"ğŸ”‘ TF-IDF í‚¤ì›Œë“œ ì¶”ì¶œ")
        print(f"{'='*60}\n")

        # TF-IDF ë²¡í„°í™”
        tfidf = TfidfVectorizer(
            max_features=50,
            ngram_range=(1, 2),
            stop_words='english'
        )

        tfidf_matrix = tfidf.fit_transform(self.texts_df['text'])
        feature_names = tfidf.get_feature_names_out()

        # í‰ê·  TF-IDF ì ìˆ˜ ê³„ì‚°
        avg_scores = np.array(tfidf_matrix.mean(axis=0)).flatten()
        keyword_scores = list(zip(feature_names, avg_scores))
        keyword_scores.sort(key=lambda x: x[1], reverse=True)

        # DataFrameìœ¼ë¡œ ë³€í™˜
        keywords_df = pd.DataFrame(keyword_scores[:30], columns=['keyword', 'tfidf_score'])
        self.keywords_df = keywords_df

        print("ğŸ“Š ìƒìœ„ 30ê°œ í‚¤ì›Œë“œ (TF-IDF ì ìˆ˜)")
        print(keywords_df.head(15).to_string(index=False))

        print(f"\nğŸ’¡ ê°€ì¥ ì¤‘ìš”í•œ í‚¤ì›Œë“œ:")
        for i in range(5):
            print(f"  {i+1}. {keywords_df.iloc[i]['keyword']}: {keywords_df.iloc[i]['tfidf_score']:.4f}")

        return keywords_df

    def perform_topic_modeling(self):
        """
        LDA í† í”½ ëª¨ë¸ë§
        """
        print(f"\n{'='*60}")
        print(f"ğŸ¯ LDA í† í”½ ëª¨ë¸ë§ (4ê°œ êµ°ì§‘)")
        print(f"{'='*60}\n")

        # CountVectorizerë¡œ ë³€í™˜
        vectorizer = CountVectorizer(
            max_features=100,
            stop_words='english',
            ngram_range=(1, 2)
        )

        doc_term_matrix = vectorizer.fit_transform(self.texts_df['text'])
        feature_names = vectorizer.get_feature_names_out()

        # LDA ëª¨ë¸
        lda = LatentDirichletAllocation(
            n_components=4,
            random_state=42,
            max_iter=20
        )

        lda.fit(doc_term_matrix)

        # ê° í† í”½ë³„ ì£¼ìš” ë‹¨ì–´ ì¶”ì¶œ
        topics = []
        for topic_idx, topic in enumerate(lda.components_):
            top_words_idx = topic.argsort()[-10:][::-1]
            top_words = [feature_names[i] for i in top_words_idx]
            topics.append({
                'topic_id': topic_idx,
                'top_keywords': ', '.join(top_words[:5]),
                'interpretation': self._interpret_topic(top_words)
            })

        topics_df = pd.DataFrame(topics)
        self.topics_df = topics_df

        print("ğŸ“Š ë°œê²¬ëœ í† í”½ (4ê°œ êµ°ì§‘)")
        for idx, row in topics_df.iterrows():
            print(f"\n  Topic {row['topic_id']}: {row['interpretation']}")
            print(f"  ì£¼ìš” í‚¤ì›Œë“œ: {row['top_keywords']}")

        return topics_df

    def _interpret_topic(self, words):
        """í† í”½ í•´ì„"""
        words_str = ' '.join(words[:5]).lower()

        if any(w in words_str for w in ['difficult', 'hard', 'confused', 'dont understand']):
            return 'í•™ìŠµ ë‚œì´ë„ / ì´í•´ ì–´ë ¤ì›€'
        elif any(w in words_str for w in ['pandas', 'numpy', 'dataframe', 'data']):
            return 'ë°ì´í„° ì²˜ë¦¬ ë„êµ¬ / ë¼ì´ë¸ŒëŸ¬ë¦¬'
        elif any(w in words_str for w in ['start', 'path', 'where', 'course']):
            return 'í•™ìŠµ ë°©í–¥ / ê²½ë¡œ íƒìƒ‰'
        elif any(w in words_str for w in ['project', 'kaggle', 'apply']):
            return 'í”„ë¡œì íŠ¸ ì‹¤ìŠµ / ì ìš©'
        else:
            return 'ê¸°íƒ€ í•™ìŠµ ì´ìŠˆ'

    def analyze_sentiment_distribution(self):
        """
        ê°ì • ë¶„í¬ ë¶„ì„ (ë‹¨ìˆœí™”ëœ ë²„ì „)
        """
        print(f"\n{'='*60}")
        print(f"ğŸ˜Š ê°ì • ë¶„í¬ ë¶„ì„")
        print(f"{'='*60}\n")

        # ê°ì • ê´€ë ¨ í‚¤ì›Œë“œ ê¸°ë°˜ ë¶„ë¥˜
        negative_keywords = ['difficult', 'hard', 'confused', 'frustrating', 'stuck', 'helpless', 'gave up']
        neutral_keywords = ['start', 'learn', 'course', 'tutorial']
        positive_keywords = ['easy', 'understand', 'solution']

        sentiments = []
        for text in self.texts_df['text']:
            neg_count = sum(1 for k in negative_keywords if k in text.lower())
            neu_count = sum(1 for k in neutral_keywords if k in text.lower())
            pos_count = sum(1 for k in positive_keywords if k in text.lower())

            if neg_count > pos_count:
                sentiment = 'Negative'
            elif pos_count > neg_count:
                sentiment = 'Positive'
            else:
                sentiment = 'Neutral'

            sentiments.append(sentiment)

        self.texts_df['sentiment'] = sentiments

        sentiment_dist = self.texts_df['sentiment'].value_counts()
        sentiment_pct = (sentiment_dist / len(self.texts_df) * 100).round(1)

        sentiment_df = pd.DataFrame({
            'sentiment': sentiment_dist.index,
            'count': sentiment_dist.values,
            'percentage': sentiment_pct.values
        })
        self.sentiment_df = sentiment_df

        print("ğŸ“Š ê°ì • ë¶„í¬")
        print(sentiment_df.to_string(index=False))

        print(f"\nğŸ’¡ ì»¤ë®¤ë‹ˆí‹° ì „ì²´ ê°ì •:")
        print(f"  - ë¶€ì •ì : {sentiment_pct.get('Negative', 0):.1f}%")
        print(f"  - ì¤‘ë¦½ì : {sentiment_pct.get('Neutral', 0):.1f}%")
        print(f"  - ê¸ì •ì : {sentiment_pct.get('Positive', 0):.1f}%")

        return sentiment_df

    def save_all_data(self, filename_prefix='nlp_painpoint_analysis'):
        """ëª¨ë“  ë°ì´í„° ì €ì¥"""
        os.makedirs('output', exist_ok=True)

        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        excel_path = f"output/{filename_prefix}_{timestamp}.xlsx"

        with pd.ExcelWriter(excel_path, engine='openpyxl') as writer:
            if hasattr(self, 'keywords_df'):
                self.keywords_df.to_excel(writer, sheet_name='TF-IDF_Keywords', index=False)

            if hasattr(self, 'topics_df'):
                self.topics_df.to_excel(writer, sheet_name='LDA_Topics', index=False)

            if hasattr(self, 'sentiment_df'):
                self.sentiment_df.to_excel(writer, sheet_name='Sentiment_Distribution', index=False)

        print(f"\nğŸ’¾ ë°ì´í„° ì €ì¥ ì™„ë£Œ:")
        print(f"  - Excel: {excel_path}")

        return excel_path


def main():
    """ì‹¤í–‰"""
    analyzer = NLPPainPointAnalyzer()

    # 1. í…ìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„
    analyzer.prepare_text_data()

    # 2. TF-IDF í‚¤ì›Œë“œ
    analyzer.extract_tfidf_keywords()

    # 3. í† í”½ ëª¨ë¸ë§
    analyzer.perform_topic_modeling()

    # 4. ê°ì • ë¶„ì„
    analyzer.analyze_sentiment_distribution()

    # 5. ì €ì¥
    excel_path = analyzer.save_all_data('kastor_nlp_analysis')

    print(f"\n{'='*60}")
    print(f"âœ… NLP ë¶„ì„ ì™„ë£Œ")
    print(f"{'='*60}\n")

    return analyzer


if __name__ == "__main__":
    main()
