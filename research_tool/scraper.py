"""
Reddit Data Scraper for Kastor Data Academy
Uses PRAW (Python Reddit API Wrapper)
"""

import praw
import pandas as pd
from datetime import datetime
import os
from dotenv import load_dotenv
from tqdm import tqdm
import time
from config import SUBREDDITS, MAX_POSTS_PER_KEYWORD, TIME_FILTER, SORT_BY

# Load environment variables
load_dotenv()

class RedditScraper:
    def __init__(self):
        """Initialize Reddit API connection"""
        self.reddit = praw.Reddit(
            client_id=os.getenv('REDDIT_CLIENT_ID'),
            client_secret=os.getenv('REDDIT_CLIENT_SECRET'),
            user_agent=os.getenv('REDDIT_USER_AGENT', 'Kastor_Research_Bot/1.0')
        )

        # Test connection
        try:
            self.reddit.user.me()
            print("âœ“ Reddit API ì—°ê²° ì„±ê³µ (ì½ê¸° ì „ìš©)")
        except:
            print("âœ“ Reddit API ì—°ê²° ì„±ê³µ (ìµëª… ëª¨ë“œ)")

    def search_subreddit(self, subreddit_name, keywords, max_posts=50):
        """
        ì„œë¸Œë ˆë”§ì—ì„œ í‚¤ì›Œë“œ ê²€ìƒ‰

        Args:
            subreddit_name: ì„œë¸Œë ˆë”§ ì´ë¦„
            keywords: ê²€ìƒ‰ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
            max_posts: í‚¤ì›Œë“œë‹¹ ìµœëŒ€ ê²Œì‹œê¸€ ìˆ˜

        Returns:
            DataFrame with collected posts
        """
        all_posts = []
        subreddit = self.reddit.subreddit(subreddit_name)

        print(f"\n{'='*60}")
        print(f"ğŸ“Š ìˆ˜ì§‘ ì¤‘: r/{subreddit_name}")
        print(f"{'='*60}")

        for keyword in tqdm(keywords, desc=f"r/{subreddit_name}"):
            try:
                # Search with keyword
                search_results = subreddit.search(
                    query=keyword,
                    sort=SORT_BY,
                    time_filter=TIME_FILTER,
                    limit=max_posts
                )

                for post in search_results:
                    post_data = {
                        'subreddit': f"r/{subreddit_name}",
                        'keyword': keyword,
                        'post_id': post.id,
                        'title': post.title,
                        'selftext': post.selftext[:500] if post.selftext else '',  # ì²« 500ìë§Œ
                        'author': str(post.author) if post.author else '[deleted]',
                        'created_utc': datetime.fromtimestamp(post.created_utc),
                        'upvotes': post.score,
                        'upvote_ratio': post.upvote_ratio,
                        'num_comments': post.num_comments,
                        'url': f"https://reddit.com{post.permalink}",
                        'collected_at': datetime.now()
                    }
                    all_posts.append(post_data)

                # Rate limiting
                time.sleep(0.5)

            except Exception as e:
                print(f"\nâš  ì˜¤ë¥˜ ({keyword}): {str(e)}")
                continue

        df = pd.DataFrame(all_posts)
        print(f"âœ“ r/{subreddit_name}: {len(df)}ê°œ ê²Œì‹œê¸€ ìˆ˜ì§‘")

        return df

    def collect_all_data(self):
        """ëª¨ë“  ì„œë¸Œë ˆë”§ì—ì„œ ë°ì´í„° ìˆ˜ì§‘"""
        all_data = []

        print(f"\n{'#'*60}")
        print(f"# Kastor Data Academy - Reddit ë°ì´í„° ìˆ˜ì§‘")
        print(f"{'#'*60}\n")

        for subreddit_name, config in SUBREDDITS.items():
            print(f"\nëª©í‘œ: {config['description']}")

            df = self.search_subreddit(
                subreddit_name,
                config['keywords'],
                MAX_POSTS_PER_KEYWORD
            )

            if not df.empty:
                df['category'] = config['description']
                all_data.append(df)

        # Combine all dataframes
        if all_data:
            combined_df = pd.concat(all_data, ignore_index=True)

            # Remove duplicates (same post found by multiple keywords)
            combined_df = combined_df.drop_duplicates(subset=['post_id'])

            print(f"\n{'='*60}")
            print(f"ğŸ“ˆ ì´ ìˆ˜ì§‘ ê²°ê³¼")
            print(f"{'='*60}")
            print(f"ì´ ê²Œì‹œê¸€: {len(combined_df)}ê°œ")
            print(f"ì„œë¸Œë ˆë”§ë³„:")
            print(combined_df.groupby('subreddit').size())

            return combined_df

        return pd.DataFrame()

    def save_raw_data(self, df, filename):
        """ì›ë³¸ ë°ì´í„° ì €ì¥"""
        if df.empty:
            print("âš  ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        # Create output directory
        os.makedirs('output', exist_ok=True)

        # Save as CSV
        csv_path = f"output/{filename}.csv"
        df.to_csv(csv_path, index=False, encoding='utf-8-sig')

        # Save as Excel (ë” ë³´ê¸° ì¢‹ìŒ)
        excel_path = f"output/{filename}.xlsx"
        df.to_excel(excel_path, index=False, engine='openpyxl')

        print(f"\nâœ“ ë°ì´í„° ì €ì¥ ì™„ë£Œ:")
        print(f"  - CSV: {csv_path}")
        print(f"  - Excel: {excel_path}")

        return csv_path, excel_path


def main():
    """ì‹¤í–‰ ì˜ˆì‹œ"""
    scraper = RedditScraper()

    # ë°ì´í„° ìˆ˜ì§‘
    df = scraper.collect_all_data()

    # ì €ì¥
    if not df.empty:
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        scraper.save_raw_data(df, f'reddit_raw_data_{timestamp}')

        return df

    return None


if __name__ == "__main__":
    main()
